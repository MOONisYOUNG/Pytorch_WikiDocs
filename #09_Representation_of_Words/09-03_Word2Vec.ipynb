{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c27bdc",
   "metadata": {},
   "source": [
    "## 👉 09-03. 워드투벡터 (Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f291ff",
   "metadata": {},
   "source": [
    "* 자연어 벡터 연산을 경험할 수 있는 사이트 : http://w.elnn.kr/search/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88704213",
   "metadata": {},
   "source": [
    "## 1. 희소 표현 (Sparse Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bf184",
   "metadata": {},
   "source": [
    "* 원-핫 인코딩의 결과로 나온 벡터들은 희소 표현에 속함.\n",
    "* 희소 표현으로는 각 단어 간의 유사성을 표현할 수 없다는 단점이 존재함.\n",
    "* 따라서 희소 표현의 단점을 해결할 수 있는 또 다른 방식이 필요함. 그것이 뒤에서 배울 분산 표현(Disttributed Representation)임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f8f5a3",
   "metadata": {},
   "source": [
    "## 2. 분산 표현 (Distributed Representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb74be5",
   "metadata": {},
   "source": [
    "* 분산 표현은 분포 가설(Distributed Hypothesis)라는 가정 하에 만들어진 표현 방법임.\n",
    "* 함께 자주 쓰이는 단어들은 의미적으로 비슷한 벡터가 됨.\n",
    "* 희소 표현은 다차원 중 한 공간에 몰아서 표현한다면, 분산 표현은 저차원에 단어 의미를 분산하여 표현함. \n",
    "* 학습 방법으로는 NNLM, RNNLM, Word2Vec(가장 많이 쓰임) 등이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb9a655",
   "metadata": {},
   "source": [
    "## 3. CBOW (Continuous Bag of Words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6507013f",
   "metadata": {},
   "source": [
    "* CBOW는 주변에 있는 단어들을 통해, 중간 단어를 예측하는 방법임.\n",
    "* CBOW를 적용하기 전에 앞서, 슬라이딩 윈도우 방식(윈도우 크기는 n이라고 가정)을 적용하여 원-핫 인코딩 처리한 데이터셋(주변 단어 2n개, 중심 단어 1개)을 구축함.\n",
    "* 은닉층이 1개로 구성되어 있는 얕은 신경망(Shallow Neural Network)를 사용함. (DNN 사용 X)\n",
    "* CBOW에 사용되는 은닉층을 투사층(Projection Layer)라고 부르기도 함.\n",
    "* '투사층의 크기'는 '희망하는 임베딩 벡터 결과물의 크기(M차원)'와 같다. \n",
    "* '입력층과 은닉층 사이를 연결하는 행렬(W)'을 룩업 테이블(Lookup Table)이라고 부른다. \n",
    "* 입력층과 은닉층 중간에서 나온 결과물 벡터들을 평균 연산한 후, '은닉층과 출력층 사이를 연결하는 행렬(W')'과 곱한다. 곱한 최종값은 소프트맥스 함수에 넣어서 스코어 벡터(Score Vector)를 구한다. \n",
    "* '스코어 벡터 값'이 '중심 단어 : 원-핫 인코딩 값'과 같아질 때까지 학습을 반복한다. 다른 말로 표현하자면, Cross-Entropy 값이 0이 될 때까지 학습을 반복하면 된다.\n",
    "* M차원 크기를 갖는 W의 행이나 W'의 열 중에서 사용할 임베딩 벡터를 고르면 됨. 또는 W와 W'의 평균치를 구해서 임베딩 벡터를 고를 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274506ac",
   "metadata": {},
   "source": [
    "## 4. Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb55ba",
   "metadata": {},
   "source": [
    "* Skip-gram은 CBOW와 유사한 방식을 사용함. 다만 '중심 단어(Input)'를 통해 '주변 단어(Output)'를 예측하기 때문에 평균을 구하는 과정은 없음.\n",
    "* 전반적인 성능은 Skip-gram이 CBOW보다 좋은 편임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb48380",
   "metadata": {},
   "source": [
    "## 5. 네거티브 샘플링 (Negative Sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fb417",
   "metadata": {},
   "source": [
    "* Word2Vec은 대부분 Skip-gram을 사용하는 경우가 많음. 근데 Skip-gram만 적용하면 불필요한 연산량이 많아지므로 Negative Sampling을 도입한 SGNS(Skip-Gram with Negative Sampling)을 사용함. \n",
    "* SGNS는 다중 클래스 분류 문제를 이진 분류 형태로 바꿈으로써 연산량을 더 효율적으로 만듦."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
